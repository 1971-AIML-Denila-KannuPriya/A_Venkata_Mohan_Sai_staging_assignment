{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label for [3.5, 2.0]: A\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def euclidean_distance(point1: List[float], point2: List[float]) -> float:\n",
    "    \"\"\"Calculate the Euclidean distance between two points.\"\"\"\n",
    "    return math.sqrt(sum((p1 - p2) ** 2 for p1, p2 in zip(point1, point2)))\n",
    "\n",
    "def knn_classifier(data_points: List[Tuple[List[float], str]], \n",
    "                   new_point: List[float], \n",
    "                   k: int = 3) -> str:\n",
    "    \"\"\"Classify the new_point using the K-Nearest Neighbors algorithm.\"\"\"\n",
    "    \n",
    "    # Step 1: Calculate distances from the new point to all data points\n",
    "    distances = []\n",
    "    for features, label in data_points:\n",
    "        distance = euclidean_distance(features, new_point)\n",
    "        distances.append((distance, label))\n",
    "    \n",
    "    # Step 2: Sort distances and get the k nearest neighbors\n",
    "    distances.sort(key=lambda x: x[0])  # Sort by distance\n",
    "    k_nearest_neighbors = distances[:k]\n",
    "    \n",
    "    # Step 3: Majority voting\n",
    "    labels = [label for _, label in k_nearest_neighbors]\n",
    "    most_common_label = Counter(labels).most_common(1)[0][0]\n",
    "    \n",
    "    return most_common_label\n",
    "\n",
    "# Example usage\n",
    "data_points = [\n",
    "    ([2.5, 2.4], 'A'),\n",
    "    ([1.0, 1.0], 'B'),\n",
    "    ([1.5, 1.7], 'A'),\n",
    "    ([5.0, 2.0], 'B'),\n",
    "    ([3.0, 2.5], 'A'),\n",
    "    ([4.5, 5.0], 'B'),\n",
    "]\n",
    "\n",
    "new_point = [3.5, 2.0]\n",
    "predicted_label = knn_classifier(data_points, new_point, k=3)\n",
    "print(f\"Predicted label for {new_point}: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Classes: [0 1]\n",
      "Class Probabilities: [0.15190364 0.98592678]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize parameters\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # Gradient descent\n",
    "        for _ in range(self.n_iterations):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "\n",
    "            # Calculate gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict_prob(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self.sigmoid(linear_model)\n",
    "        return y_predicted\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        y_predicted_prob = self.predict_prob(X)\n",
    "        y_predicted_class = [1 if prob >= threshold else 0 for prob in y_predicted_prob]\n",
    "        return np.array(y_predicted_class)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample dataset\n",
    "    X = np.array([[0.1, 0.5], [1.0, 1.2], [1.5, 1.8], [5.0, 5.5], [6.0, 6.1]])\n",
    "    y = np.array([0, 0, 0, 1, 1])  # Binary labels\n",
    "\n",
    "    # Create logistic regression model\n",
    "    model = LogisticRegression(learning_rate=0.1, n_iterations=1000)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Make predictions\n",
    "    new_data = np.array([[2.0, 2.0], [5.5, 5.0]])\n",
    "    predictions = model.predict(new_data)\n",
    "    probabilities = model.predict_prob(new_data)\n",
    "\n",
    "    print(\"Predicted Classes:\", predictions)\n",
    "    print(\"Class Probabilities:\", probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19, 22], [43, 50]]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def matrix_multiply(mat1: List[List[int]], mat2: List[List[int]]) -> List[List[int]]:\n",
    "    if len(mat1[0]) != len(mat2):\n",
    "        raise ValueError(\"Incompatible matrices for multiplication.\")\n",
    "\n",
    "    result = [[0 for _ in range(len(mat2[0]))] for _ in range(len(mat1))]\n",
    "    for i in range(len(mat1)):\n",
    "        for j in range(len(mat2[0])):\n",
    "            for k in range(len(mat2)):\n",
    "                result[i][j] += mat1[i][k] * mat2[k][j]\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "mat1 = [[1, 2], [3, 4]]\n",
    "mat2 = [[5, 6], [7, 8]]\n",
    "print(matrix_multiply(mat1, mat2))  # Output: [[19, 22], [43, 50]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.1, 0.2], [0.3, 0.4]], [[0.5, 0.6], [0.7, 0.8]]]]\n"
     ]
    }
   ],
   "source": [
    "def deep_copy_network(network: List[List[List[float]]]) -> List[List[List[float]]]:\n",
    "    return [[layer.copy() for layer in network]]\n",
    "\n",
    "# Example usage\n",
    "network = [[[0.1, 0.2], [0.3, 0.4]], [[0.5, 0.6], [0.7, 0.8]]]\n",
    "copied_network = deep_copy_network(network)\n",
    "print(copied_network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving Average: [1.0, 1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def moving_average(series: List[float], window_size: int) -> List[float]:\n",
    "    \"\"\"Calculate the moving average of a time series.\"\"\"\n",
    "    if window_size <= 0:\n",
    "        raise ValueError(\"Window size must be a positive integer.\")\n",
    "    if len(series) == 0:\n",
    "        return []\n",
    "\n",
    "    moving_averages = []\n",
    "    for i in range(len(series)):\n",
    "        # Determine the window start and end indices\n",
    "        start_index = max(0, i - window_size + 1)\n",
    "        end_index = i + 1  # end is exclusive\n",
    "        # Calculate the average for the current window\n",
    "        window = series[start_index:end_index]\n",
    "        window_average = sum(window) / len(window)\n",
    "        moving_averages.append(window_average)\n",
    "\n",
    "    return moving_averages\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    time_series = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    window_size = 3\n",
    "    result = moving_average(time_series, window_size)\n",
    "    print(\"Moving Average:\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU Outputs: [1, 3, 5, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def relu(inputs):\n",
    "    \n",
    "    return [max(0, x) for x in inputs]\n",
    "\n",
    "\n",
    "inputs=[1,3,5,-1,-2,-3]\n",
    "outputs = relu(inputs)\n",
    "print(\"ReLU Outputs:\", outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Model: y = 2.1860 * x + -0.9494\n",
      "Predictions: [1.2365492606419035, 3.422547414710774, 5.608545568779644, 7.794543722848514, 9.980541876917384]\n",
      "Mean Squared Error: 0.5604651301247262\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def mean_squared_error(y_true: List[float], y_pred: List[float]) -> float:\n",
    "    \"\"\"Calculate Mean Squared Error.\"\"\"\n",
    "    return sum((yt - yp) ** 2 for yt, yp in zip(y_true, y_pred)) / len(y_true)\n",
    "\n",
    "def linear_regression_gradient_descent(X: List[List[float]], y: List[float], \n",
    "                                       learning_rate: float = 0.01, \n",
    "                                       iterations: int = 1000) -> Tuple[float, float]:\n",
    "    \"\"\"Train a linear regression model using gradient descent.\"\"\"\n",
    "    # Initialize weights (m and b)\n",
    "    m = 0.0  # slope\n",
    "    b = 0.0  # intercept\n",
    "    n = len(y)  # number of data points\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        # Calculate predictions\n",
    "        y_pred = [m * x + b for x in X]\n",
    "        \n",
    "        # Calculate gradients\n",
    "        m_gradient = (-2/n) * sum(x * (yt - yp) for x, yt, yp in zip(X, y, y_pred))\n",
    "        b_gradient = (-2/n) * sum(yt - yp for yt, yp in zip(y, y_pred))\n",
    "        \n",
    "        # Update weights\n",
    "        m -= learning_rate * m_gradient\n",
    "        b -= learning_rate * b_gradient\n",
    "        \n",
    "    return m, b\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data (X: feature, y: target)\n",
    "    X = [1, 2, 3, 4, 5]  # Independent variable\n",
    "    y = [2, 3, 5, 7, 11]  # Dependent variable\n",
    "\n",
    "    # Train the model\n",
    "    slope, intercept = linear_regression_gradient_descent(X, y)\n",
    "\n",
    "    print(f\"Trained Model: y = {slope:.4f} * x + {intercept:.4f}\")\n",
    "    \n",
    "    # Predictions using the trained model\n",
    "    y_pred = [slope * x + intercept for x in X]\n",
    "    print(\"Predictions:\", y_pred)\n",
    "    print(\"Mean Squared Error:\", mean_squared_error(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1], [1, 0], [1, 0], [0, 1]]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def one_hot_encode(labels: List[str]) -> List[List[int]]:\n",
    "    unique_labels = list(set(labels))\n",
    "    label_index = {label: i for i, label in enumerate(unique_labels)}\n",
    "    \n",
    "    one_hot = [[0] * len(unique_labels) for _ in range(len(labels))]\n",
    "    for i, label in enumerate(labels):\n",
    "        one_hot[i][label_index[label]] = 1\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "# Example usage\n",
    "labels = ['cat', 'dog', 'dog', 'cat']\n",
    "print(one_hot_encode(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9746318461970762\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import math\n",
    "\n",
    "def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:\n",
    "    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
    "    magnitude1 = math.sqrt(sum(a ** 2 for a in vec1))\n",
    "    magnitude2 = math.sqrt(sum(b ** 2 for b in vec2))\n",
    "    return dot_product / (magnitude1 * magnitude2) if magnitude1 and magnitude2 else 0.0\n",
    "\n",
    "# Example usage\n",
    "vec1 = [1, 2, 3]\n",
    "vec2 = [4, 5, 6]\n",
    "print(cosine_similarity(vec1, vec2))  # Output: 0.9746318461970762\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.1, 0.05], [0.03, 0.2]], [[0.2], [0.5]]]\n"
     ]
    }
   ],
   "source": [
    "def prune_network(network: List[List[List[float]]], threshold: float = 0.01) -> List[List[List[float]]]:\n",
    "    return [[[weight for weight in layer if abs(weight) > threshold] for layer in net] for net in network]\n",
    "\n",
    "# Example usage\n",
    "network = [[[0.1, 0.05], [0.03, 0.2]], [[0.01, 0.2], [0.5, 0]]]\n",
    "print(prune_network(network))  # Output: [[[0.1, 0.05], [0.2]], [[0.2], [0.5]]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1], [1, 1]]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def confusion_matrix(true_labels: List[int], predicted_labels: List[int]) -> List[List[int]]:\n",
    "    matrix = [[0, 0], [0, 0]]  # True Negatives, False Positives, False Negatives, True Positives\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        matrix[true][pred] += 1\n",
    "    return matrix\n",
    "\n",
    "# Example usage\n",
    "true_labels = [0, 0, 1, 1]\n",
    "predicted_labels = [0, 1, 0, 1]\n",
    "print(confusion_matrix(true_labels, predicted_labels))  # Output: [[1, 1], [1, 1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized weights: [1.0097699148143664]\n",
      "Optimized bias: 0.971088109305324\n",
      "Predicted value for input 5: 6.019937683377156\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "import random\n",
    "\n",
    "def predict(X: List[float], weights: List[float], bias: float) -> float:\n",
    "    \"\"\"Predicts the output for given inputs X using the model's weights and bias.\"\"\"\n",
    "    return sum(w * x for w, x in zip(weights, X)) + bias\n",
    "\n",
    "def mini_batch_gradient_descent(X: List[List[float]], y: List[float], batch_size: int, \n",
    "                                 learning_rate: float = 0.01, iterations: int = 1000) -> Tuple[List[float], float]:\n",
    "    \"\"\"\n",
    "    Perform mini-batch gradient descent to optimize linear regression weights.\n",
    "\n",
    "    Args:\n",
    "        X: List of input features.\n",
    "        y: List of target values.\n",
    "        batch_size: Size of mini-batches for gradient descent.\n",
    "        learning_rate: Learning rate for weight updates.\n",
    "        iterations: Number of iterations to run.\n",
    "\n",
    "    Returns:\n",
    "        Optimized weights and bias.\n",
    "    \"\"\"\n",
    "    m, n = len(X), len(X[0])\n",
    "    weights = [0.0] * n\n",
    "    bias = 0.0\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        # Shuffle the dataset\n",
    "        indices = list(range(m))\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        for i in range(0, m, batch_size):\n",
    "            # Create mini-batch\n",
    "            batch_indices = indices[i:i + batch_size]\n",
    "            X_batch = [X[j] for j in batch_indices]\n",
    "            y_batch = [y[j] for j in batch_indices]\n",
    "\n",
    "            # Predictions for the mini-batch\n",
    "            predictions = [predict(x, weights, bias) for x in X_batch]\n",
    "\n",
    "            # Calculate gradients\n",
    "            gradients_w = [0.0] * n\n",
    "            gradients_b = 0.0\n",
    "\n",
    "            for j in range(len(X_batch)):\n",
    "                error = predictions[j] - y_batch[j]\n",
    "                for k in range(n):\n",
    "                    gradients_w[k] += error * X_batch[j][k]\n",
    "                gradients_b += error\n",
    "\n",
    "            # Average gradients\n",
    "            gradients_w = [gw / len(X_batch) for gw in gradients_w]\n",
    "            gradients_b /= len(X_batch)\n",
    "\n",
    "            # Update weights and bias\n",
    "            weights = [w - learning_rate * gw for w, gw in zip(weights, gradients_w)]\n",
    "            bias -= learning_rate * gradients_b\n",
    "\n",
    "    return weights, bias\n",
    "\n",
    "# Example usage\n",
    "X = [[1], [2], [3], [4]]  # Feature matrix (4 samples, 1 feature)\n",
    "y = [2, 3, 4, 5]  # Target values (y = x + 1)\n",
    "batch_size = 2\n",
    "weights, bias = mini_batch_gradient_descent(X, y, batch_size)\n",
    "\n",
    "print(\"Optimized weights:\", weights)\n",
    "print(\"Optimized bias:\", bias)\n",
    "\n",
    "# Making predictions\n",
    "test_point = [5]\n",
    "predicted_value = predict(test_point, weights, bias)\n",
    "print(\"Predicted value for input 5:\", predicted_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "def euclidean_distance(point1: Tuple[float, float], point2: Tuple[float, float]) -> float:\n",
    "    return sum((a - b) ** 2 for a, b in zip(point1, point2)) ** 0.5\n",
    "\n",
    "def k_means_clustering(data_points: List[Tuple[float, float]], k: int) -> List[int]:\n",
    "    # Randomly initialize centroids\n",
    "    centroids = random.sample(data_points, k)\n",
    "    clusters = [0] * len(data_points)\n",
    "    for _ in range(100):  # Max iterations\n",
    "        # Assign clusters\n",
    "        for i, point in enumerate(data_points):\n",
    "            clusters[i] = min(range(k), key=lambda j: euclidean_distance(point, centroids[j]))\n",
    "        \n",
    "        # Update centroids\n",
    "        new_centroids = []\n",
    "        for j in range(k):\n",
    "            points_in_cluster = [data_points[i] for i in range(len(data_points)) if clusters[i] == j]\n",
    "            new_centroids.append(tuple(sum(p) / len(p) for p in zip(*points_in_cluster)))\n",
    "        centroids = new_centroids\n",
    "    return clusters\n",
    "\n",
    "# Example usage\n",
    "data_points = [(1, 2), (2, 3), (3, 1), (8, 9), (9, 8)]\n",
    "print(k_means_clustering(data_points, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057317038045, 0.24472847105479764, 0.6652409557748219]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import math\n",
    "\n",
    "def softmax(scores: List[float]) -> List[float]:\n",
    "    exp_scores = [math.exp(score) for score in scores]\n",
    "    sum_exp_scores = sum(exp_scores)\n",
    "    return [exp / sum_exp_scores for exp in exp_scores]\n",
    "\n",
    "# Example usage\n",
    "scores = [1.0, 2.0, 3.0]\n",
    "print(softmax(scores))  # Output: [0.09003057317038046, 0.24472847105479767, 0.6652409557748219]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'the': 0.0, 'cat': 0.11552453009332421, 'sat': 0.0, 'on': 0.0, 'mat': 0.11552453009332421}, {'the': 0.0, 'dog': 0.11552453009332421, 'sat': 0.0, 'on': 0.0, 'log': 0.11552453009332421}]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def tf_idf(documents: List[str]) -> List[dict]:\n",
    "    tf = []\n",
    "    df = Counter()\n",
    "    num_docs = len(documents)\n",
    "    \n",
    "    for doc in documents:\n",
    "        words = doc.split()\n",
    "        doc_counter = Counter(words)\n",
    "        tf_doc = {word: count / len(words) for word, count in doc_counter.items()}\n",
    "        tf.append(tf_doc)\n",
    "        for word in doc_counter:\n",
    "            df[word] += 1\n",
    "\n",
    "    tfidf = []\n",
    "    for tf_doc in tf:\n",
    "        tfidf_doc = {word: tf_doc[word] * math.log(num_docs / df[word]) for word in tf_doc}\n",
    "        tfidf.append(tfidf_doc)\n",
    "    \n",
    "    return tfidf\n",
    "\n",
    "# Example usage\n",
    "docs = [\"the cat sat on the mat\", \"the dog sat on the log\"]\n",
    "print(tf_idf(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.82797019]\n",
      " [-1.77758033]\n",
      " [ 0.99219749]\n",
      " [ 0.27421042]\n",
      " [ 1.67580142]\n",
      " [ 0.9129491 ]\n",
      " [-0.09910944]\n",
      " [-1.14457216]\n",
      " [-0.43804614]\n",
      " [-1.22382056]]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "def pca(data: List[List[float]], num_components: int):\n",
    "    # Center the data\n",
    "    data_meaned = data - np.mean(data, axis=0)\n",
    "    # Covariance matrix\n",
    "    cov_matrix = np.cov(data_meaned, rowvar=False)\n",
    "    # Eigenvalues and Eigenvectors\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "    # Sort by eigenvalues\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices][:, :num_components]\n",
    "    return np.dot(data_meaned, eigenvectors)\n",
    "\n",
    "# Example usage\n",
    "data = [[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2, 1.6], [1, 1.1], [1.5, 1.6], [1.1, 0.9]]\n",
    "print(pca(np.array(data), 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def calculate_auc_roc(true_labels: List[int], predicted_probs: List[float]) -> float:\n",
    "    return roc_auc_score(true_labels, predicted_probs)\n",
    "\n",
    "# Example usage\n",
    "true_labels = [0, 1, 1, 0]\n",
    "predicted_probs = [0.1, 0.4, 0.35, 0.8]\n",
    "print(calculate_auc_roc(true_labels, predicted_probs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.6, 0, 0.8]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import List\n",
    "\n",
    "def apply_dropout(layer_outputs: List[float], dropout_rate: float) -> List[float]:\n",
    "    return [output if random.random() > dropout_rate else 0 for output in layer_outputs]\n",
    "\n",
    "# Example usage\n",
    "outputs = [0.5, 0.6, 0.7, 0.8]\n",
    "print(apply_dropout(outputs, 0.2))  # Randomly sets some outputs to 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.414213562373095, -0.7071067811865475, 0.0, 0.7071067811865475, 1.414213562373095]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def z_score_normalization(data: List[float]) -> List[float]:\n",
    "    mean = sum(data) / len(data)\n",
    "    variance = sum((x - mean) ** 2 for x in data) / len(data)\n",
    "    std_dev = variance ** 0.5\n",
    "    return [(x - mean) / std_dev for x in data]\n",
    "\n",
    "# Example usage\n",
    "data = [10, 20, 30, 40, 50]\n",
    "print(z_score_normalization(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.164252033486018\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import math\n",
    "\n",
    "def cross_entropy_loss(true_labels: List[int], predicted_probs: List[float]) -> float:\n",
    "    return -sum(true * math.log(pred) + (1 - true) * math.log(1 - pred) for true, pred in zip(true_labels, predicted_probs)) / len(true_labels)\n",
    "\n",
    "# Example usage\n",
    "true_labels = [0, 1, 1, 0]\n",
    "predicted_probs = [0.1, 0.9, 0.8, 0.2]\n",
    "print(cross_entropy_loss(true_labels, predicted_probs))  # Output will vary based on predicted_probs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SEABORN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
